{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male  파일 길이 :  582\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (295).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (450).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (588).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (113).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (577).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (535).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (378).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (453).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (79).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (399).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (587).jpg\n",
      "male  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/male/male_ (305).jpg\n",
      "female  파일 길이 :  564\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (242).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (509).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (216).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (175).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (592).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (165).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (454).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (190).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (488).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (596).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (117).jpg\n",
      "female  :  /home/yukim/Machine Learning/human_classification/face_classification/data/train/female/female_ (363).jpg\n",
      "ok 1146\n"
     ]
    }
   ],
   "source": [
    "# 필요한 패키지 불러오기\n",
    "\n",
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 학습시킬 데이터 불러오기\n",
    "\n",
    "caltech_dir = \"./data/train\"\n",
    "categories = [\"male\", \"female\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# 데이터 전처리\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    # one-hot 인코딩\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 학습 데이터 테스트 데이터를 나눈뒤 np배열형식 외부파일로 저장\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./numpy_data/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(859, 64, 64, 3)\n",
      "859\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./numpy_data/multi_image_data.npy')\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"male\",\"female\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "# 일반화\n",
    "\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 설계하기\n",
    "\n",
    "with K.tf_ops.device('/device:GPU:0'): # 시스템의 첫번째 GPU를 지정함\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 4,214,466\n",
      "Trainable params: 4,214,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 설계된 신경망 자세히 살펴보기\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 859 samples, validate on 287 samples\n",
      "Epoch 1/50\n",
      "859/859 [==============================] - 5s 6ms/step - loss: 0.9778 - acc: 0.6193 - val_loss: 0.6586 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65864, saving model to ./model/multi_img_classification.model\n",
      "Epoch 2/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.5696 - acc: 0.7113 - val_loss: 0.5086 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65864 to 0.50861, saving model to ./model/multi_img_classification.model\n",
      "Epoch 3/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.4426 - acc: 0.7974 - val_loss: 0.3536 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50861 to 0.35356, saving model to ./model/multi_img_classification.model\n",
      "Epoch 4/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.2948 - acc: 0.8836 - val_loss: 0.2393 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35356 to 0.23931, saving model to ./model/multi_img_classification.model\n",
      "Epoch 5/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.2195 - acc: 0.9185 - val_loss: 0.1976 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23931 to 0.19761, saving model to ./model/multi_img_classification.model\n",
      "Epoch 6/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.1781 - acc: 0.9395 - val_loss: 0.1876 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.19761 to 0.18765, saving model to ./model/multi_img_classification.model\n",
      "Epoch 7/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.1721 - acc: 0.9325 - val_loss: 0.1961 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.18765\n",
      "Epoch 8/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.1295 - acc: 0.9499 - val_loss: 0.1519 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18765 to 0.15186, saving model to ./model/multi_img_classification.model\n",
      "Epoch 9/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.1146 - acc: 0.9499 - val_loss: 0.1450 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.15186 to 0.14497, saving model to ./model/multi_img_classification.model\n",
      "Epoch 10/50\n",
      "859/859 [==============================] - 5s 6ms/step - loss: 0.1107 - acc: 0.9546 - val_loss: 0.1454 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14497\n",
      "Epoch 11/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.1028 - acc: 0.9604 - val_loss: 0.1348 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14497 to 0.13483, saving model to ./model/multi_img_classification.model\n",
      "Epoch 12/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0818 - acc: 0.9732 - val_loss: 0.1565 - val_acc: 0.9408\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13483\n",
      "Epoch 13/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0782 - acc: 0.9651 - val_loss: 0.1445 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13483\n",
      "Epoch 14/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0616 - acc: 0.9779 - val_loss: 0.1300 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13483 to 0.12997, saving model to ./model/multi_img_classification.model\n",
      "Epoch 15/50\n",
      "859/859 [==============================] - 5s 6ms/step - loss: 0.0498 - acc: 0.9825 - val_loss: 0.1290 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.12997 to 0.12896, saving model to ./model/multi_img_classification.model\n",
      "Epoch 16/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0585 - acc: 0.9814 - val_loss: 0.1190 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.12896 to 0.11903, saving model to ./model/multi_img_classification.model\n",
      "Epoch 17/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0505 - acc: 0.9790 - val_loss: 0.1310 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11903\n",
      "Epoch 18/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0369 - acc: 0.9872 - val_loss: 0.1139 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11903 to 0.11387, saving model to ./model/multi_img_classification.model\n",
      "Epoch 19/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0342 - acc: 0.9872 - val_loss: 0.1047 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11387 to 0.10468, saving model to ./model/multi_img_classification.model\n",
      "Epoch 20/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0360 - acc: 0.9860 - val_loss: 0.1986 - val_acc: 0.9408\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10468\n",
      "Epoch 21/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0380 - acc: 0.9825 - val_loss: 0.1175 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10468\n",
      "Epoch 22/50\n",
      "859/859 [==============================] - 5s 6ms/step - loss: 0.0308 - acc: 0.9884 - val_loss: 0.1504 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10468\n",
      "Epoch 23/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0432 - acc: 0.9872 - val_loss: 0.1264 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10468\n",
      "Epoch 24/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0317 - acc: 0.9919 - val_loss: 0.1771 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10468\n",
      "Epoch 25/50\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.0446 - acc: 0.9825 - val_loss: 0.1508 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10468\n"
     ]
    }
   ],
   "source": [
    "# 설계한 모델 훈련시키기\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 0s 771us/step\n",
      "정확도 : 0.9477\n"
     ]
    }
   ],
   "source": [
    "# 훈련 정확도\n",
    "\n",
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 정확도 손실값 시각화\n",
    "\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (333).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (115).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (361).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (72).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (425).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (49).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (422).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (426).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (74).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (195).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (68).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (373).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (196).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (479).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (178).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (59).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (82).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (405).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (54).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (244).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (269).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (510).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (501).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (451).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (349).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (120).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (571).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (221).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (264).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (451).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (351).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (263).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (135).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (182).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (460).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (603).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (360).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (159).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (33).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (186).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (313).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (359).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (101).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (209).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (543).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (536).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (390).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (227).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (350).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (365).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (369).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (144).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (258).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (304).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (139).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (343).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (487).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (115).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (46).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (311).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (184).jpg이미지는 여자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (34).jpg이미지는 남자로 예측합니다.\n",
      "[1.000 0.000]\n",
      "0\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/male_ (109).jpg이미지는 남자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (173).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (540).jpg이미지는 여자로 예측합니다.\n",
      "[0.000 1.000]\n",
      "1\n",
      "해당 /home/yukim/Machine Learning/human_classification/face_classification/data/test_dir/female_ (586).jpg이미지는 여자로 예측합니다.\n"
     ]
    }
   ],
   "source": [
    "# 훈련된 모델로 예측값 검증하기\n",
    "\n",
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "caltech_dir = \"./data/test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "model = load_model('./model/multi_img_classification.model')\n",
    "\n",
    "prediction = model.predict(X)\n",
    "\n",
    "# numpy float 출력 옵션 변경 - 사용자가 포맷팅을 이용하여 원하는대로 배열 요소를 표시 가능\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax() \n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0:\n",
    "        pre_ans_str = \"남자\"\n",
    "    else:\n",
    "        pre_ans_str = \"여자\"\n",
    "    for k in range(len(i)):    \n",
    "        if i[k] >= 0.8: \n",
    "            print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 예측합니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
